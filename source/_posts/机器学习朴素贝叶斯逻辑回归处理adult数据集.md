---
title: æœºå™¨å­¦ä¹ å®éªŒæœ´ç´ è´å¶æ–¯é€»è¾‘å›å½’å¤„ç†adultæ•°æ®é›†(é™„ä»£ç )
date: 2021-11-26 09:34:24
tags:
    - å­¦ä¹ 
    - æœºå™¨å­¦ä¹ 
    - python
---
# æ”¾åœ¨æœ€å‰é¢
- é¢˜ç›®æ¥è‡ªå¹¿å·å¤§å­¦æœºå™¨å­¦ä¹ å®éªŒ
- æœ¬æ–‡ä»¥è§£å†³æ•°æ®é›†å¤„ç†é—®é¢˜ä¸ºæœ€é«˜ç›®æ ‡ï¼Œä¸ä¼šå¯¹æ¶‰åŠåˆ°çš„çŸ¥è¯†è¿‡å¤šä»‹ç»ï¼Œä¼šå†™å‡ºé—®é¢˜è§£å†³æ€è·¯æ–¹å¼
# å®éªŒé¢˜ç›®
>åŸºäºAdultæ•°æ®é›†ï¼Œå®Œæˆå…³äºæ”¶å…¥æ˜¯å¦å¤§äº50Kçš„é€»è¾‘å›å½’åˆ†ç±»ã€æœ´ç´ è´å¶æ–¯æ¨¡å‹è®­ç»ƒã€æµ‹è¯•ä¸è¯„ä¼°ã€‚
- 1 å‡†å¤‡æ•°æ®é›†å¹¶è®¤è¯†æ•°æ®
    ä¸‹è½½Adultæ•°æ®é›†
    http://archive.ics.uci.edu/ml/datasets/Adult
    äº†è§£æ•°æ®é›†å„ä¸ªç»´åº¦ç‰¹å¾åŠé¢„æµ‹å€¼çš„å«ä¹‰
- 2 æ¢ç´¢æ•°æ®å¹¶é¢„å¤„ç†æ•°æ®
	è§‚å¯Ÿæ•°æ®é›†å„ä¸ªç»´åº¦ç‰¹å¾åŠé¢„æµ‹å€¼çš„æ•°å€¼ç±»å‹ä¸åˆ†å¸ƒ
    é¢„å¤„ç†å„ç»´åº¦ç‰¹å¾
- 3 è®­ç»ƒæ¨¡å‹
	ç¼–ç¨‹å®ç°è®­ç»ƒæ•°æ®é›†ä¸Šé€»è¾‘å›å½’æ¨¡å‹çš„æ¢¯åº¦ä¸‹é™å‚æ•°æ±‚è§£ã€æœ´ç´ è´å¶æ–¯å‚æ•°ç»Ÿè®¡
- 4 æµ‹è¯•å’Œè¯„ä¼°æ¨¡å‹
 	åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè®¡ç®—æ‰€è®­ç»ƒæ¨¡å‹çš„å‡†ç¡®ç‡ã€AUCç­‰æŒ‡æ ‡

# å®éªŒå†…å®¹åŠæ­¥éª¤
## æ•°æ®é›†ä¸‹è½½å¤„ç†
    http://archive.ics.uci.edu/ml/datasets/Adult
ä¸‹è½½åç”¨excelæ‰“å¼€æ–‡ä»¶ï¼Œå‘ç°æ²¡æœ‰indexæ²¡æœ‰åˆ—åï¼Œåªå¥½æ‰‹åŠ¨å¤„ç†æ·»åŠ ä¸€ä¸‹ï¼Œå¤„ç†å¥½çš„æ–‡ä»¶æˆ‘ä¼šæ”¾åœ¨æºç æ–‡ä»¶å¤¹é‡Œã€‚
![](https://cdn.jsdelivr.net/gh/HK560/MyPicHub@master/res/pic/20211126101609.png)

ä½¿ç”¨pandasè¯»å–æ–‡ä»¶,ç®€å•å¤„ç†ä¸€ä¸‹
```python
# è¯»å–æ•°æ®,é¢„å¤„ç†
df = pd.read_csv('adult.csv') 
#å»æ‰å«æœ‰nançš„è¡Œ
df=df.dropna(axis=0,how='any')
```
è§‚å¯Ÿæ•°æ®å‘ç°,æœ‰äº›åœ°æ–¹æœ‰ç¼ºçœå€¼,ç”¨"?"è¡¨ç¤º,æˆ‘ä»¬éœ€è¦å»æ‰è¿™äº›å«æœ‰é—®å·çš„è¡Œ
```python
#é¢„å¤„ç†ï¼Œå»æ‰ç¼ºçœå±æ€§çš„è¡Œ
df=df[~df['workclass'].isin(["?"])]
df=df[~df['native-country'].isin(["?"])]
df=df[~df['occupation'].isin(["?"])]
```
å› ä¸ºæ˜¯è¦åˆ†ç±»æ”¶å…¥å¤§äº50Kå’Œå°äºç­‰äº50Kçš„,æˆ‘ä»¬ç›´æ¥å¤„ç†ä¸€ä¸‹,å°†å¤§äº50kçš„è®¾ä¸º1,å°äºç­‰äº50kçš„è®¾ä¸º0,æ”¾åœ¨æ–°çš„ä¸€ä¸ªåˆ—,åˆ—åtarget
```python
#å°†ç»“æœè½¬ä¸ºäºŒåˆ†ç±»å€¼ä¸º0 1æ–¹ä¾¿åç»­å¤„ç†ã€‚é‡å‘½åä¸ºtargetåˆ—
df.loc[df['income'] == ">50K","target"] = 1
df.loc[df['income'] != ">50K","target"] = 0
df.drop(['income'], axis=1 ,inplace=True)
```
### 1.é€»è¾‘å›å½’åˆ†ç±»
ç»§ç»­å¾€ä¸‹é¢çœ‹å‰å»ºè®®å…ˆæ‰§è¡Œäº†è§£ä¸€ä¸‹é€»è¾‘å›å½’æ˜¯ä»€ä¹ˆä¸œè¥¿,è¿™é‡Œä¸å†ç»†è¯´å±•å¼€,ç›´æ¥å†™ä»£ç 

é¢„å¤„ç†å®Œä¹‹å,ä¸æœ´ç´ è´å¶æ–¯ä¸åŒ
æˆ‘ä»¬è¿˜éœ€è¦å°†ç¦»æ•£å‹ç‰¹å¾å’Œè¿ç»­æ€§ç‰¹å¾ç­›é€‰å‡ºæ¥,è¿›è¡Œå¤„ç†
```python
#é€‰å‡ºç¦»æ•£å‹ç‰¹å¾
cat_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 
               'gender','native-country']
#è¿ç»­æ€§ç‰¹å¾
num_columns = ['age','fnlwgt','educational-num','capital-gain', 'capital-loss', 'hours-per-week']
#ç›®æ ‡ç‰¹å¾
target_column = "target"
#å¯¹ç¦»æ•£å‹ç‰¹å¾ç‹¬çƒ­ç¼–ç 
encoded_df = pd.get_dummies(df,columns=cat_columns)

#ç‰¹å¾å€¼
df_x = encoded_df.drop(columns="target")
#ç›®æ ‡å€¼
df_y = encoded_df["target"].values
```
ç„¶åæŠŠè¿ç»­å‹ç‰¹å¾çš„å€¼å½’ä¸€åŒ–
```python
#å°†è¿ç»­å‹ç‰¹å¾å€¼å½’ä¸€åŒ–
num_mean = df_x[num_columns].mean()
num_std = df_x[num_columns].std()
num_normol = (df_x[num_columns] - num_mean)/num_std
df_x.drop(columns=num_columns,inplace=True)
df_x = pd.concat([df_x,num_normol],axis=1).values   
```
ç„¶åå°±å¯ä»¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†äº†,ä½¿ç”¨sklearnçš„åº“åˆ’åˆ†
```python
#åˆ’åˆ†è®­ç»ƒé›†æµ‹è¯•é›†
from sklearn.model_selection import StratifiedShuffleSplit
sss = StratifiedShuffleSplit(n_splits=2,train_size=0.7)
for train_index,test_index in sss.split(df_x,df_y):
    trainx,testx = df_x[train_index],df_x[test_index]
    trainy,testy = df_y[train_index],df_y[test_index]
```
æ¥ä¸‹æ¥æˆ‘ä»¬å°±è¦ç€æ‰‹å®ç°é€»è¾‘å›å½’äº†,ä¸äº†è§£é€»è¾‘å›å½’çš„å¼ºçƒˆå»ºè®®å…ˆæ‰§è¡Œå­¦ä¹ ä¸€ä¸‹

å®ç°sigmoidå‡½æ•°
```python
#sigmoidå‡½æ•°
def sigmoid(x):
  return 1 / (1 + np.exp(-x))
```
æ¢¯åº¦ä¸‹é™æ³•çš„é€»è¾‘å›å½’å®ç°

```python
X=trainx
Y=np.reshape(trainy, (-1, 1))
X_test=testx
Y_test=np.reshape(testy, (-1, 1))
# print(X.shape)
# print (Y.shape)

theta=np.zeros(shape=[1,X.shape[1]])
b=0
#è®¾ç½®å­¦ä¹ é€Ÿç‡
alpha= 0.001
#å­¦ä¹ æ¬¡æ•°
learning_count =10000
for i in range(learning_count):
    #y_h æ˜¯é¢„æµ‹çš„yå€¼
    y_h=np.dot(X,theta.T) + b
    #æ±‚å‡ºæŸå¤±
    lost=y_h-Y
    theta=theta - alpha*(1/len(X))*np.dot(lost.T,X)
    #åšæ¢¯åº¦ä¸‹é™
    b=b-alpha*(1/len(X))* lost.sum()
    #ä»£ä»·
    cost = (lost**2)/(len(X))
    if i%10000==0:
        print ('æ¢¯åº¦ä¸‹é™ä¸­...')


y_pre= np.dot(X_test,theta.T)+b
y_preSigmd=sigmoid(y_pre)
```
è¿™æ ·æ±‚å‡ºçš„é¢„æµ‹å€¼å¹¶ä¸èƒ½ç›´æ¥å’Œæµ‹è¯•å€¼æ¯”è¾ƒ,æˆ‘ä»¬ä»¥0.5åˆ’åˆ†æ˜¯0æ˜¯1
```python
y_preSigmd= np.where(y_preSigmd>0.5,1,0)
```
å¥½è¿™æ ·y_preSigmodé‡Œå°±æ˜¯æˆ‘ä»¬æ ¹æ®æµ‹è¯•é›†é¢„æµ‹å‡ºæ¥çš„å€¼

è®¡ç®—ä¸€ä¸‹å‡†ç¡®ç‡
```python
from sklearn.metrics import accuracy_score
print("å‡†ç¡®ç‡ï¼š",accuracy_score(y_preSigmd, Y_test))
```
AUCå€¼:
```python
from sklearn.metrics import roc_auc_score
auc_score = roc_auc_score(Y_test,y_preSigmd)
print("AUCå€¼:",auc_score)
```
### 2.æœ´æ ‘è´å¶æ–¯åˆ†ç±»

é¢„å¤„ç†å®Œä¹‹å,ä¸é€»è¾‘å›å½’ä¸åŒ,æˆ‘ä»¬éœ€è¦ç€é‡å¤„ç†è¿ç»­å‹ç‰¹å¾,å°†å…¶åˆ’åˆ†ä¸ºå‡ ä¸ªåŒºé—´ç‰¹å¾
```python
#å°†è¿ç»­å‹ç‰¹å¾åˆ’åˆ†åŒºé—´
df['age']=pd.cut(df['age'],bins=10,right=True)
df['capital-gain']=pd.cut(df['capital-gain'],bins=3,right=True)
df['capital-loss']=pd.cut(df['capital-loss'],bins=3,right=True)
df['hours-per-week']=pd.cut(df['hours-per-week'],bins=20,right=True)
df['fnlwgt']=pd.cut(df['fnlwgt'],bins=20,right=True)
```
å…¶ä»–æœ¬èº«å°±å·²æ˜¯ç¦»æ•£å‹çš„ç‰¹å¾å°±ä¸éœ€è¦å¤„ç†äº†

ç»§ç»­åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
```python
#åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†,è®¾ç½®å˜é‡,ä¸ºä¸‹é¢è®­ç»ƒåšå‡†å¤‡
train_data=df.sample(frac=0.7, random_state=0,axis=0)
test_data=df[~df.index.isin(train_data.index)]

df_x = df.drop(columns="target")
df_y = df["target"].values

trainX=train_data.drop(columns="target")
trainY=train_data['target']
testX=test_data.drop(columns="target")
testY=test_data['target']

```
æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å®ç°æœ´æ ‘è´å¶æ–¯,ä»¥ä¸‹ä»£ç æ¥è‡ªä»–äººä¹‹æ‰‹,è¿›è¡Œäº†å®Œå–„ä¿®æ”¹
```python
class NaiveBayes(object):
    def __init__(self, X_train, y_train):
        self.X_train = X_train  #æ ·æœ¬ç‰¹å¾
        self.y_train = y_train  #æ ·æœ¬ç±»åˆ«
        #è®­ç»ƒé›†æ ·æœ¬ä¸­æ¯ä¸ªç±»åˆ«(äºŒåˆ†ç±»)çš„å æ¯”ï¼Œå³P(ç±»åˆ«)ï¼Œä¾›åç»­ä½¿ç”¨
        self.P_label = {1: np.mean(y_train.values), 0: 1-np.mean(y_train.values)}
 
    #åœ¨æ•°æ®é›†dataä¸­, ç‰¹å¾featureçš„å€¼ä¸ºvalueçš„æ ·æœ¬æ‰€å æ¯”ä¾‹
    #ç”¨äºè®¡ç®—P(ç‰¹å¾|ç±»åˆ«)ã€P(ç‰¹å¾)
    def getFrequency(self, data, feature, value):
        num = len(data[data[feature]==value]) #ä¸ªæ•°
        return num / (len(data))
 
    def predict(self, X_test):
        self.prediction = [] #é¢„æµ‹ç±»åˆ«
        # éå†æ ·æœ¬
        for i in range(len(X_test)):
            x = X_test.iloc[i]      # ç¬¬iä¸ªæ ·æœ¬
            P_feature_label0 = 1    # P(ç‰¹å¾|ç±»åˆ«0)ä¹‹å’Œ
            P_feature_label1 = 1    # P(ç‰¹å¾|ç±»åˆ«1)ä¹‹å’Œ
            P_feature = 1           # P(ç‰¹å¾)ä¹‹å’Œ
            # éå†ç‰¹å¾
            for feature in X_test.columns:
                # åˆ†å­é¡¹ï¼ŒP(ç‰¹å¾|ç±»åˆ«)
                data0 = self.X_train[self.y_train.values==0]  #å–ç±»åˆ«ä¸º0çš„æ ·æœ¬
                P_feature_label0 *= self.getFrequency(data0, feature, x[feature]) #è®¡ç®—P(feature|0)
 
                data1 = self.X_train[self.y_train.values==1]  #å–ç±»åˆ«ä¸º1çš„æ ·æœ¬
                P_feature_label1 *= self.getFrequency(data1, feature, x[feature]) #è®¡ç®—P(feature|1)
 
                # åˆ†æ¯é¡¹ï¼ŒP(ç‰¹å¾)
                P_feature *= self.getFrequency(self.X_train, feature, x[feature])
 
            #å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡
            if i%25==0:
                # print ('...')
                print("é¢„æµ‹ç¬¬",i,"ä¸ª")
                print(P_feature_label0)
                print(P_feature_label1)
            P_0 = (P_feature_label0*self.P_label[0]) / P_feature
            P_1 = (P_feature_label1 * self.P_label[1]) / P_feature
            #é€‰å‡ºå¤§æ¦‚ç‡å€¼å¯¹åº”çš„ç±»åˆ«
            self.prediction.append([1 if P_1>=P_0 else 0])
        return self.prediction
#from https://blog.csdn.net/weixin_39710249/article/details/111373540
```
è°ƒç”¨è¯¥ç±»è®­ç»ƒé¢„æµ‹
```python
model = NaiveBayes(trainX, trainY)  
y_pre = model.predict(testX)    
print(y_pre)     
```
è®¡ç®—å‡†ç¡®ç‡:
```python
from sklearn.metrics import accuracy_score
print("å‡†ç¡®ç‡ï¼š",accuracy_score(y_pre, testY))
```
è®¡ç®—aucå€¼:
```python
#aucå€¼
from sklearn.metrics import roc_auc_score
auc_score = roc_auc_score(testY,y_pre)
print("AUCå€¼:",auc_score)
```
# æºç æ–‡ä»¶
 [Github](https://github.com/HK560/MyMachineLearning/tree/main/experiment2)
ç­‰å®éªŒäº¤äº†æˆ‘å†è¡¥æºç ~~~ğŸ˜

# Reference
- [æœ´ç´ è´å¶æ–¯ä»£ç _ã€Šæœºå™¨å­¦ä¹ ã€‹ä¹‹ æœ´ç´ è´å¶æ–¯åŸç†åŠä»£ç ](https://blog.csdn.net/weixin_39710249/article/details/111373540)
- [é€»è¾‘å›å½’æ¨¡å‹é¢„æµ‹æˆå¹´äººæ”¶å…¥æ°´å¹³](https://zhuanlan.zhihu.com/p/45835659)
- [åŸºäºå†³ç­–æ ‘å’Œæœ´ç´ è´å¶æ–¯ç®—æ³•å¯¹Adultæ•°æ®é›†åˆ†ç±»](https://blog.csdn.net/hohaizx/article/details/79084774)